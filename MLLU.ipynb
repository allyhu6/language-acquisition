{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLLU.ipynb",
      "provenance": [],
      "mount_file_id": "1mqtYFtTknG0OLsoUUV2PVF6vjHOwXWLd",
      "authorship_tag": "ABX9TyP7nmyss6gyl4BIMFdELifH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allyhu6/language-acquisition/blob/main/MLLU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzpsMQevtjhA"
      },
      "source": [
        "#to find he directory to unzip the data\n",
        "import os\n",
        "import pandas as pd\n",
        "os.listdir()\n",
        "\n",
        "cwd = os.getcwd()\n",
        "\n",
        "\n",
        "import zipfile\n",
        "with zipfile.ZipFile(cwd + \"/Brown.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(cwd)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQhRYG0jdcqa"
      },
      "source": [
        "#iterating through the files and putting utterances into one file\n",
        "import numpy as np\n",
        "directory = \"Brown\"\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "  if filename in ['Adam', 'Eve', 'Sarah']:\n",
        "    for file in os.listdir(directory + '/' + filename):\n",
        "      #print(file)\n",
        "      child_age = file[0:4]\n",
        "      if file.endswith(\".cha\"):\n",
        "          f = open(directory + '/' + filename +'/' + file)\n",
        "          lines = f.read()\n",
        "          fileOutput = open('alldata.csv','a')\n",
        "          fileLines = lines.split('\\n')\n",
        "          for line in fileLines: # loop through each entry of that array (i.e., each line)\n",
        "            if len(line)>0 and line[0:4] == '*CHI': # we only want the child tier\n",
        "              line = line.replace('(', '')\n",
        "              line = line.replace(')', '')\n",
        "              lineContent = line.split('\\t')[1] # second element of array, using tab character\n",
        "              #lineWords = lineContent.split(' ') # get all words (split by space)\n",
        "              fileOutput.write(str(child_age)+ ',' + str(lineContent) +'\\n')\n",
        "              \n",
        "    else:\n",
        "      fileOutput.close()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apnKSUzn5FZP"
      },
      "source": [
        "#fp = open(\"alldata.csv\")\n",
        "#content = fp.read()\n",
        "#print(content)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6rgqLfG79MS"
      },
      "source": [
        "#splitting the datafram and dropping lines where the words are indiscernable \n",
        "df = pd.read_csv(\"alldata.csv\",names=[\"age\", \"utterance\"])\n",
        "\n",
        "#print(df.dtypes)\n",
        "df[~df.utterance.str.contains(\"xxx\")]\n",
        "df[~df.utterance.str.contains(\"yyy\")]\n",
        "df['age'] = df['age'].astype(str)\n",
        "df[df['age'].str.isnumeric()]\n",
        "#print(df['age'].value_counts())\n",
        "\n",
        "ages = []\n",
        "for age in df['age']:\n",
        "  if len(age) > 1:\n",
        "    if int(age[:2]) >= 5:\n",
        "      ages.append(int(age[:1]) + 1)\n",
        "  else:\n",
        "    ages.append(int(age[:1]))\n",
        "\n",
        "df['age'] = ages\n",
        "df = df.sample(frac = 1)\n",
        "\n",
        "testing = df.sample(frac = .3)\n",
        "training = df.sample(frac = .7)\n",
        "\n",
        "testing.to_csv(\"test.csv\")\n",
        "training.to_csv(\"train.csv\")"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqqETCiyySl-",
        "outputId": "377362ee-1e90-46bd-f6ac-7f3cf21018f4"
      },
      "source": [
        "df['age'].value_counts()\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3    201795\n",
              "4    168801\n",
              "5    109884\n",
              "2     44379\n",
              "6      7743\n",
              "Name: age, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb0sPTZ8U5sQ",
        "outputId": "c3340332-511b-4e9e-cf73-fa7af09239bc"
      },
      "source": [
        "! pip install pytorch_transformers\n",
        "\n",
        "from fastai.text import *\n",
        "from fastai.metrics import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import RobertaTokenizer\n",
        "from pytorch_transformers import RobertaModel"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.0.45)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (0.1.95)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers) (1.17.73)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch_transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (8.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch_transformers) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.73 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_transformers) (1.20.73)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_transformers) (0.4.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_transformers) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.73->boto3->pytorch_transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LkOQmfnKrff",
        "outputId": "a1a6a318-590f-442e-800b-c5c216508c3a"
      },
      "source": [
        "\n",
        "# To install the package \"pytorch-transformers\"\n",
        "! pip install pytorch-transformers pendulum\n",
        "\n",
        "import pendulum\n",
        "from fastai.text import *\n",
        "from fastai.metrics import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import RobertaTokenizer\n",
        "from pytorch_transformers import RobertaModel\n",
        "\n",
        "# Garbage Collector\n",
        "import gc "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "Requirement already satisfied: pendulum in /usr/local/lib/python3.7/dist-packages (2.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.19.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.17.73)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (4.41.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (0.1.95)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2.23.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (1.8.1+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: python-dateutil<3.0,>=2.6 in /usr/local/lib/python3.7/dist-packages (from pendulum) (2.8.1)\n",
            "Requirement already satisfied: pytzdata>=2020.1 in /usr/local/lib/python3.7/dist-packages (from pendulum) (2020.1)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (0.4.2)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.73 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (1.20.73)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-transformers) (0.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers) (1.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmvpVrgEX56r"
      },
      "source": [
        "#config part\n",
        "class Config(dict):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    for k, v in kwargs.items():\n",
        "      setattr(self, k, v)\n",
        "\n",
        "  def set(self, key, val):\n",
        "    self[key] = val\n",
        "    setattr(self, key, val)\n",
        "\n",
        "config = Config(\n",
        "  seed = 18,\n",
        "  roberta_model_name = 'roberta-large',\n",
        "  max_lr = 1e-5,\n",
        "  epochs = 1,\n",
        "  max_seq_len = 400,\n",
        "  bs = 4,\n",
        "  valid_pct = .2,\n",
        "  hidden_dropout_prob = .05,\n",
        "  hidden_size = 1024,\n",
        "  num_labels = 5,\n",
        "  start_tok = \"<s>\",\n",
        "  end_tok = \"</s>\",\n",
        "  model_path = 'Model_Roberta.pkl',\n",
        "  pred_path = 'Prediction_Roberta.csv',\n",
        "  train_file_path = 'train.csv',\n",
        "  test_file_path = 'test.csv',\n",
        "  text_column_name = 'utterance',\n",
        "  target_column_name = 'age'\n",
        ")"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjZJTxm09xhI"
      },
      "source": [
        "\n",
        "class FastAiRobertaTokenizer(BaseTokenizer):\n",
        "  def __init__(self, tokenizer: RobertaTokenizer, max_seq_len: int=128, **kwargs):\n",
        "    self._pretrained_tokenizer = Tokenizer()\n",
        "    self.max_seq_len = max_seq_len\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    return self\n",
        "  def tokenizer(self, t:str) -> List[str]:\n",
        "    return [config.start_tok] + self._pretrained_tokenizer.process_text(t, BaseTokenizer(lang=\"en\"))[:self.max_seq_len - 2] + [config.end_tok]\n",
        "\n",
        "\n",
        "roberta_tok = RobertaTokenizer.from_pretrained(config.roberta_model_name)\n",
        "fastai_tokenizer = Tokenizer(tok_func = FastAiRobertaTokenizer(roberta_tok, max_seq_len = config.max_seq_len), pre_rules=[], post_rules=[])\n",
        "\n",
        "path = Path()\n",
        "roberta_tok.save_vocabulary(path)\n",
        "\n",
        "with open('vocab.json', 'r') as f:\n",
        "    roberta_vocab_dict = json.load(f)\n",
        "\n",
        "fastai_roberta_vocab = Vocab(list(roberta_vocab_dict.keys()))\n",
        "\n",
        "class RobertaTokenizerProcessor(TokenizeProcessor):\n",
        "  def __init__(self, tokenizer):\n",
        "    super().__init__(tokenizer=tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "class RobertaNumericalProcessor(NumericalizeProcessor):\n",
        "  def __init__(self, *args, **kwards):\n",
        "    super().__init__(*args, vocab=fastai_roberta_vocab, **kwards)\n",
        "\n",
        "\n",
        "def get_roberta_processor(tokenizer: Tokenizer=None, vocab:Vocab=None):\n",
        "  return [RobertaTokenizerProcessor(tokenizer=tokenizer), NumericalizeProcessor(vocab=vocab)]\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "2moNdbodL6cq",
        "outputId": "2d87074d-56d3-47a2-ee43-b6546865ad0f"
      },
      "source": [
        "class RobertaDataBunch(TextDataBunch):\n",
        "  @classmethod\n",
        "  def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', bs:int=64, val_bs:int=None, pad_idx=1,\n",
        "    pad_first=True, decive:torch.device=None, no_check:bool=False, backwards:bool=False,\n",
        "    l_tfms:Optional[Collection[Callable]]=None, **dl_kwargs) -> DataBunch:\n",
        "    datasets = cls.__init__ds(train_ds, valid_ds, test_ds)\n",
        "    val_bs = ifnone(val_bs, bs)\n",
        "    collate_fn = partial(pad_collate, pad_idx=pad_idx, pad_first=pad_first, backwards=backwards)\n",
        "    train_sampler = SortishSample(dataset[0].x, key=lambda t:len(datasets[0][t][0].data), bs=bs)\n",
        "    train_dl = Dataloader(datasets[0], batch_size=bs, sample= train_sampler, drop_last = True, **dl_kwargs)\n",
        "    dataloaders = [train_dl]\n",
        "    for ds in datasets[1:]:\n",
        "      lengths = [len(t) for t in ds.x.items]\n",
        "      sampler = SortSampler(ds.x, key = lengths.__getitem__)\n",
        "      dataloaders.append(DataLoader(ds, batch_size=val_bs, sampler=sampler, **dl_kwargs))\n",
        "    return cls(*dataloaders, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)\n",
        "\n",
        "class RobertaTextList(TextList):\n",
        "  _bunch = RobertaDataBunch\n",
        "  _label_cls = TextList\n",
        "\n",
        "\n",
        "train_df = pd.read_csv(config.train_file_path)\n",
        "test_df = pd.read_csv(config.test_file_path)\n",
        "feat_cols = config.text_column_name\n",
        "label_cols = config.target_column_name\n",
        "\n",
        "\n",
        "\n",
        "processor = get_roberta_processor(\n",
        "    tokenizer = fastai_tokenizer, vocab=fastai_roberta_vocab\n",
        ")\n",
        "\n",
        "data = TextList.from_df(train_df, \".\", cols=feat_cols, processor=processor)\\\n",
        "  .split_by_rand_pct(valid_pct=config.valid_pct, seed=config.seed)\\\n",
        "  .label_from_df(cols=label_cols, label_cls=CategoryList)\\\n",
        "  .add_test(RobertaTextList.from_df(test_df, \".\", cols=feat_cols, processor=processor))\\\n",
        "  .databunch(bs=config.bs, pad_first=False, pad_idx=0)\n",
        "\n",
        "\n",
        "del train_df\n",
        "del test_df\n",
        "gc.collect()\n",
        "\n"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/fastai/core.py:302: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return np.array(a, dtype=dtype, **kwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwjoe-HmHSG2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "6a7bcd2e-52a7-4489-a221-9f5909794570"
      },
      "source": [
        "# defining our model architecture\n",
        "class CustomRobertaModel(nn.Module):\n",
        "    def __init__(self, num_labels=5):\n",
        "        super(CustomRobertaModel, self).__init__()\n",
        "        self.num_labels = num_labels  # get number of labels\n",
        "        self.roberta = RobertaModel.from_pretrained(\n",
        "            config.roberta_model_name)  # get pre-trained model\n",
        "        # set up percentage of drop\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # defining final output layer\n",
        "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.roberta(\n",
        "            input_ids, token_type_ids, attention_mask)\n",
        "        #print(pooled_output.size())\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "\n",
        "roberta_model = CustomRobertaModel()\n",
        "learn = Learner(data, roberta_model, metrics=[accuracy]) #  use acc as evaluation metrics\n",
        "learn = learn.to_fp16()  # train using half precision (instead of float 32) which can help to speedup\n",
        "\n",
        "gc.collect() # clean the memory before modelling\n",
        "learn.model.roberta.train() # setting roberta to train as it is in eval mode by default\n",
        "learn.fit_one_cycle(config.epochs, max_lr=config.max_lr) # train on one epoch\n",
        "learn.export(config.model_path)  # store the model"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.317725</td>\n",
              "      <td>1.328352</td>\n",
              "      <td>0.377971</td>\n",
              "      <td>5:48:15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n",
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFBXxqN3a0bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "5dc5f8d2-73a2-439e-a246-6a33d9d57cf3"
      },
      "source": [
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "  learn.model.roberta.eval()\n",
        "  preds = learn.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "  sampler = [i for i in data.dl(ds_type).sampler]\n",
        "  reverse_sampler = np.argsort(sampler)\n",
        "  ordered_preds = preds[reverse_sampler, :]\n",
        "  pred_values = np.argmax(ordered_preds, axis=1)\n",
        "  return ordered_preds, pred_values\n",
        "\n",
        "test_preds = get_preds_as_nparray(DatasetType.Test)\n",
        "\n",
        "test_df = pd.read_csv(config.test_file_path)\n",
        "\n",
        "prediction = test_preds[1] + 1\n",
        "test_df[config.text_column_name] = prediction\n",
        "test_df.drop([config.target_column_name], axis=1, inplace=True)\n",
        "test_df.to_csv(config.pred_path, index=False)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy8voJf2ZiWG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}